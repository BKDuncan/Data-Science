{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Part II: Neural Networks & Deep Learning\n",
    "   \n",
    "<img src=\"res/book.jpg\" width = 25% align = \"right\">\n",
    "   \n",
    "---\n",
    "CH 15 - Autoencoders\n",
    "---\n",
    "\n",
    "---\n",
    "**CH 15 - Autoencoders** **<---  <span style=\"color: #FF0000\">THIS WEEK !</span>**\n",
    "\n",
    "\n",
    "---\n",
    "**CH 16 - Reinforcement Learning** <--- <span style=\"color: #0000FF\">NEXT WEEK</span>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an Autoencoder?\n",
    "\n",
    "Lets start with a dataset of many features that we feed into a network. In the next layer, we are going to **choke up** the number of neurons and limit the amount of information it can store. Then finally we will have an output layer that fans out so it has the **same number of outputs** as there were **inputs**. \n",
    "\n",
    "<img src=\"./images/autoencoder.png\" />\n",
    "\n",
    "The networks goal is to output the **same** input after all the loss of information in the middle. If it can do that accurately then it has **learned a pattern** from the dataset that can be exploited!\n",
    "\n",
    "This is an autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders are great at uncovering patterns from unsupervised data. \n",
    "\n",
    "### Use Cases\n",
    "- dimensionality reduction\n",
    "- feature extraction \n",
    "- unsupervised pretraining\n",
    "- generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is always composed of two parts: an **encoder** (or recognition network) that converts the inputs to an internal representation, followed by a **decoder** (or generative network) that converts the internal representation to the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/undercomplete_autoencoder.png\" />\n",
    "\n",
    "Because the internal representation has a **lower dimensionality** than the input data (it is 2D instead of 3D), the autoencoder is said to be **undercomplete**. An undercomplete autoencoder cannot trivially copy its inputs to the codings, yet it must find a way to output a copy of its inputs. It is **forced to learn** the most important features in the input data (and drop the unimportant ones). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Autoencoder... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pca_autoencoder_output.png\" />\n",
    "\n",
    "### PCA Dimension Reduction using an Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_inputs = 3  # 3D inputs \n",
    "n_hidden = 2  # 2D codings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = n_inputs # Important! \n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs]) \n",
    "hidden = fully_connected(X, n_hidden, activation_fn=None) #  activation_fn=None -> Learning is Linear\n",
    "outputs = fully_connected(hidden, n_outputs, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))  # MSE\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) \n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = [...] # load the dataset\n",
    "n_iterations = 1000 \n",
    "codings = hidden  # the output of the hidden layer provides the codings\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    init.run()    \n",
    "    for iteration in range(n_iterations):        \n",
    "        training_op.run(feed_dict={X: X_train})  # no labels (unsupervised)    \n",
    "        codings_val = codings.eval(feed_dict={X: X_test}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoders (Deep Autoencoders)\n",
    "<img src=\"./images/stacked_autoencoder.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders can have multiple hidden layers. In this case they are called stacked autoencoders (or deep autoencoders). \n",
    "\n",
    "The architecture of a stacked autoencoder is typically symmetrical with regards to the central hidden layer (the coding layer). To put it simply, it looks like a sandwich. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques\n",
    "\n",
    "### Tying Weights\n",
    "\n",
    "Since autoencoders are symmetrical, we can 'tie' together the weights in the encoder and decoder layers. What this means is we can share the weights in similar layers. This halves the number of weights, speeding up our training and reducing the risk of overfitting (less degree's of freedom & complexity).\n",
    "\n",
    "\n",
    "### Training One Autoencoder at a Time\n",
    "\n",
    "Training an entire stacked autoencoder can take a long time. It turns out training them separately and then assembling them after is much faster with the same results.\n",
    "\n",
    "#### Phase 1. Train a simple Autoencoder\n",
    "\n",
    "The first autoencoder learns to reconstruct the inputs\n",
    "\n",
    "<img src=\"./images/phase-1.png\" />\n",
    "\n",
    "#### Phase 2. Train an Inner Autoencoder (Repeat for however many nested autoencoders...)\n",
    "\n",
    "The second autoencoder learns to reconstruct the output of the first autoencoder’s hidden layer\n",
    "\n",
    "<img src=\"./images/phase-2.png\" />\n",
    "\n",
    "#### Phase 3. Encoders Assemble\n",
    "\n",
    "Stack all the autoencoders\n",
    "\n",
    "<img src=\"./images/phase-3.png\" />\n",
    "\n",
    "<img src=\"./images/one-at-a-time.png\" />\n",
    "\n",
    "\n",
    "### Unsupervised Pretraining with Stacked Autoencoders\n",
    "\n",
    "We can reuse the layers in a trained autoencoder to quickly make a neural network that understands patterns in an unlabelled dataset.\n",
    "\n",
    "<img src=\"./images/pretrain-autoencoder.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Type of Autoencoders:\n",
    "\n",
    "#### Denoising Autoencoders\n",
    "The autoencoder is trained to remove noise from inputs (by adding noise to the input data but evaluating it based on noiseless inputs). This method is good for making sure the autoencoder is learning patterns and not memorizing your data, since memorizing the noisy image inhibits its performance. They usually add Gaussian Noise to the inputs or use dropout to achieve this.\n",
    "\n",
    "#### Sparse Autoencoders\n",
    "\n",
    "<img src='./images/kl_divergence_sparsity_loss.PNG' />\n",
    "These autoencoders limit the number of active neurons allowed at a time pushing the autoencoder to represent each input with fewer activiations. To do this you need to measure the **mean activation per neuron** and then penalize neurons that are too active by adding a **sparsity loss** to the cost function.\n",
    "\n",
    "#### Variational Autoencoders (VAE)\n",
    "These are **generative autoencoders**, meaning that they can generate new instances that look like they were sampled from the training set. They use Gaussian noise to crea\n",
    "\n",
    "<img src='./images/variational_autoencoder.PNG' />\n",
    "\n",
    "Instead of directly producing a coding for a given input, the encoder produces a **mean coding (μ)** and a **standard deviation (σ)**. The actual coding is then sampled randomly from a Gaussian distribution with mean μ and standard deviation σ.\n",
    "\n",
    "<img src='./images/variational_autoencoder2.PNG' />\n",
    "\n",
    "#### Contractive autoencoder (CAE)\n",
    "The autoencoder is constrained during training so that the derivatives of the codings with regards to the inputs are small. In other words, two similar inputs must have similar codings\n",
    "\n",
    "#### Stacked convolutional autoencoders\n",
    "Autoencoders that learn to extract visual features by reconstructing images processed through convolutional layers. \n",
    "\n",
    "#### Generative stochastic network (GSN)\n",
    "A generalization of denoising autoencoders, with the added capability to generate data. \n",
    "\n",
    "#### Winner-take-all (WTA) autoencoder\n",
    "During training, after computing the activations of all the neurons in the coding layer, only the top k% activations for each neuron over the training batch are preserved, and the rest are set to zero. Naturally this leads to sparse codings. Moreover, a similar WTA approach can be used to produce sparse convolutional autoencoders. \n",
    "\n",
    "#### Adversarial autoencoders\n",
    "One network is trained to reproduce its inputs, and at the same time another is trained to find inputs that the first network is unable to properly reconstruct. This pushes the first autoencoder to learn robust codings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
