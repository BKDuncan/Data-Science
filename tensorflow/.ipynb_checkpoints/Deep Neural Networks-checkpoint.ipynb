{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks...\n",
    "\n",
    "### There are tons of Deep Neural Network architectures! But some are better than others for specific problems.\n",
    "<br/><img src=\"images/book.jpg\" width = 70%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/neural-networks.png\" width = 100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So Lets pick a use case to focus on the architectures that are useful... Language Translation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/google%20translate.PNG\" width = 100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://translate.google.ca/\n",
    "\n",
    "### So... lets breakdown some of the architectures and preprocessing we need to do before we get started with translation and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN) for Generating Natural Language \n",
    "<br/>\n",
    "<img src=\"images/RNN-NLP.gif\" width=\"100%\">\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why RNN's?\n",
    "### A RNN treats each word of a sentence as a separate input occurring at time ‘t’ and uses the activation value at ‘t-1’ also, as an input in addition to the input at time ‘t’. Because each word is associated with a time, the network has memory of the past inputs and can better interpret the sentence as a whole.\n",
    "\n",
    "### However RNN's have weaknesses. For example they are great for short term memory (sentences) but will struggle to keep longer term memory (Vanishing Gradient Problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation:\n",
    "## A simple Sequence 2 Sequence architecture using RNN's\n",
    "<br/>\n",
    "<img src=\"images/sec2sec.jpeg\" width=\"100%\">\n",
    "\n",
    "### For a sentence, each word is fed into our RNN along with the output of the last word. The complete sentence is completely translated into a vector (Context Vector) by the \"Encoder\". Now the vector is fed into the \"Decoder\" RNN which does the reverse operation on the vector but with a different language.\n",
    "\n",
    "### This Encoder/Decoder Architecture is called Sequence 2 Sequence\n",
    "\n",
    "### These RNN cells are usually LSTM (Long-Short Term Memory) or GRU (Gated Recurrent Unit) which we will learn about in the Recurrent Neural network chapter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/seq2seq.gif\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/seq2seq2.gif\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Attention\n",
    "### A critical and apparent disadvantage of this fixed-length context vector design is its incapability of remembering long sentences. Often it has forgotten the first part once it completes processing the whole input. (https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#whats-wrong-with-seq2seq-model)\n",
    "\n",
    "### To deal with this vanishing gradient problem and give the network more memory, we add a mechanism called Attention!?! (http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).\n",
    "\n",
    "### The attention mechanism was born to help memorize long source sentences in neural machine translation (NMT). Rather than building a single context vector out of the encoder’s last hidden state, the secret sauce invented by attention is to create shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/attention.gif\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3\n",
    "\n",
    "# Attention Mechanism in Detail:\n",
    "<br/>\n",
    "\n",
    "## Step 1: Our Normal Sequence 2 Sequence RNN Network\n",
    "\n",
    "<img src=\"images/1.gif\" width=\"100%\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Step 2: Add Attention Scores to each Word\n",
    "\n",
    "<img src=\"images/2.gif\" width=\"100%\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Step 3: Use Softmax for a Weight\n",
    "\n",
    "<img src=\"images/3.gif\" width=\"100%\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Step 4: Multiply the Score by our Weights\n",
    "\n",
    "<img src=\"images/4.gif\" width=\"100%\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Step 5: Take the weighted sum of our Attention\n",
    "\n",
    "<img src=\"images/5.gif\" width=\"100%\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Step 6: Feed the sum into our decoder layer so it knows more about past and future words used in the input\n",
    "\n",
    "<img src=\"images/6.gif\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google's Neural Machine Translation System (AKA Google Translate)\n",
    "<img src=\"images/google-nmt.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
