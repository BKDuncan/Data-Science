{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Part II: Neural Networks & Deep Learning\n",
    "   \n",
    "<img src=\"res/book.jpg\" width = 25% align = \"right\">\n",
    "   \n",
    "---\n",
    "**CH 12 - Distributing TensorFlow Across Devices and Servers <---  <span style=\"color: #FF0000\">THIS WEEK !</span>**\n",
    "\n",
    "---\n",
    "**CH 13 - Convolutional Neural Networks <--- <span style=\"color: #0000FF\">NEXT WEEK</span>**\n",
    "\n",
    "---\n",
    "**CH 14 - Recurrent Neural Networks**\n",
    "\n",
    "---\n",
    "**CH 15 - Autoencoders**\n",
    "\n",
    "---\n",
    "**CH 16 - Reinforcement Learning**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing Tensorflow Across Devices and Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### By default tensorflow will run everything on your CPU\n",
    "<img src=\"./images/cpu.png\" width=\"20%\" align=\"right\"/>\n",
    "\n",
    "- This is quite slow\n",
    "\n",
    "- Your CPU will be busy with the OS, all your background tasks at the same time \n",
    "\n",
    "- We need a way to speed up our model training so it doesn't take days or even weeks to train something good!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what can we do to speed it up?\n",
    "\n",
    "### - Tensorflow was built to scale!\n",
    "### - Because each node in a neural network is a separate operation, it can be run on a different device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/deep_neural_network.png\"/>\n",
    "\n",
    "**Example Deep Neural Network:** with 5 hidden layers, 4 inputs, and 3 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use a GPU\n",
    "\n",
    "<img src=\"./images/gpu.png\" width=\"35%\" align=\"right\"/>\n",
    "\n",
    "### Setup\n",
    "- Uninstall your old tensorflow\n",
    "- Install tensorflow-gpu `pip install tensorflow-gpu`\n",
    "- Install Nvidia Graphics Card & Drivers (you probably already have)\n",
    "- Download & Install CUDA\n",
    "- Download & Install cuDNN\n",
    "- Verify:\n",
    "```python\n",
    "from tensorflow.python.client import device_lib \n",
    " print(device_lib.list_local_devices())\n",
    "    ```\n",
    " \n",
    "### 'Pin' Some (or All) Nodes to the GPU\n",
    "- This will assign a node to belong to a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):    \n",
    "    pi = tf.Variable(3.1415926535897932384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the rest on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):    \n",
    "    a = tf.Variable(3.0)    \n",
    "    b = tf.constant(4.0)\n",
    "c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: A GPU can only perform operations that it has a kernel (operation declaration) for! \n",
    "- For example, it doesn't have a kernel for Integers variables!\n",
    "- Use **soft placement** to automatically send nodes back to the CPU if they aren't defined for the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto() \n",
    "config.allow_soft_placement = True \n",
    "sess = tf.Session(config=config) \n",
    "sess.run(i.initializer)  # the placer runs and falls back to /cpu:0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And thats it! Tensorflow will run the nodes in parallel across all the devices.\n",
    "#### This works for more devices if you have them (`/gpu:1`, `/gpu:2`...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use multiple computers with many GPU's and CPU's as well. But things get more complicated...\n",
    "\n",
    "- We need to make a cluster of tensorflow servers and divide our work evenly!\n",
    "\n",
    "- The hardest part is choosing how to divide your work, and data among many devices\n",
    "\n",
    "**Official Tensorflow Summary with Code:**\n",
    "\n",
    "https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/terms.PNG\" />\n",
    "\n",
    "#### Client\n",
    "\n",
    "A client is typically a program that builds a TensorFlow graph and constructs a `tensorflow::Session` to interact with a cluster. Clients are typically written in Python or C++. A single client process can directly interact with multiple TensorFlow servers, and a single server can serve multiple clients.\n",
    "\n",
    "<img src=\"./images/client-server.PNG\" />\n",
    "\n",
    "#### Cluster\n",
    "\n",
    "A TensorFlow cluster comprises one or more \"jobs\", each divided into lists of one or more \"tasks\". A cluster is typically dedicated to a particular high-level objective, such as training a neural network, using many machines in parallel. A cluster is defined by a `tf.train.ClusterSpec` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spec = tf.train.ClusterSpec({    \n",
    "    \"ps\": [\n",
    "        \"machine-a.example.com:2221\",  # /job:ps/task:0    \n",
    "    ],    \n",
    "    \"worker\": [        \n",
    "        \"machine-a.example.com:2222\",  # /job:worker/task:0        \n",
    "        \"machine-b.example.com:2222\",  # /job:worker/task:1    \n",
    "    ]\n",
    "}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job\n",
    "\n",
    "A job comprises a list of \"tasks\", which typically serve a common purpose. For example, a job named `ps` (for \"parameter server\") typically hosts nodes that store and update variables; while a job named `worker` typically hosts stateless nodes that perform compute-intensive tasks. The tasks in a job typically run on different machines. The set of job roles is flexible: for example, a `worker` may maintain some state.\n",
    "\n",
    "<img src=\"./images/job-task-cluster.PNG\"/>\n",
    "\n",
    "#### Task\n",
    "\n",
    "A task corresponds to a specific TensorFlow server, and typically corresponds to a single process. A task belongs to a particular \"job\" and is identified by its index within that job's list of tasks.\n",
    "\n",
    "#### TensorFlow server \n",
    "\n",
    "A process running a `tf.train.Server` instance, which is a member of a cluster, and exports a \"master service\" and \"worker service\".\n",
    "\n",
    "<img src=\"./images/server-labelled.PNG\"/>\n",
    "\n",
    "#### Master service\n",
    "\n",
    "A service that provides remote access to a set of distributed devices, and acts as a session target. The master service implements the tensorflow::Session interface, and is responsible for coordinating work across one or more \"worker services\". All TensorFlow servers implement the master service.\n",
    "\n",
    "#### Worker service\n",
    "\n",
    "A service that executes parts of a TensorFlow graph using its local devices. All TensorFlow servers implement the worker service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we split the work in our servers?\n",
    "\n",
    "- #### There are too many ways to split the work, and it is sensitive to what kind of network you are using!\n",
    "- #### Can split your network by layer or horizontally per worker and split your data into mini-batches so you aren't waiting on other servers too long\n",
    "- #### Can train a whole neural network on each server with different hyperparameters when you are looking for the best\n",
    "- #### Can distribute training one neural network across all servers by programatically pinning nodes to each GPU/CPU on each server  in a round robin way, and then share the updating parameters from each on a parameter server (with a queue to eliminate race conditions)\n",
    "- #### Can shard your data and train different neural networks on different servers and then ensemble them together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
