{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Part II: Neural Networks & Deep Learning\n",
    "   \n",
    "<img src=\"res/book.jpg\" width = 25% align = \"right\">\n",
    "   \n",
    "---\n",
    "**CH 10 - Introduction to Artificial Neural Networks  <---  <span style=\"color: #FF0000\">THIS WEEK !</span>**\n",
    "\n",
    "---\n",
    "**CH 11 - Training Deep Neural Nets** <--- <span style=\"color: #0000FF\">NEXT WEEK</span>\n",
    "\n",
    "---\n",
    "**CH 12 - Distributing TensorFlow Across Devices and Servers**\n",
    "\n",
    "---\n",
    "**CH 13 - Convolutional Neural Networks**\n",
    "\n",
    "---\n",
    "**CH 14 - Recurrent Neural Networks**\n",
    "\n",
    "---\n",
    "**CH 15 - Autoencoders**\n",
    "\n",
    "---\n",
    "**CH 16 - Reinforcement Learning**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Taken from the neuron cell found in the brain \n",
    "<img src=\"res/Neuron.png\" width = 40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Artificial Neuron\n",
    "   - Introduced by *Warden McCulloch* (Neurophysicist) and *Walter Pitts* (Mathematician)\n",
    "   - Simplified and abstracted using propositional logic\n",
    "       - made up of 1 or more binary inputs and 1 binary output\n",
    "       - activates its output based on the number of true inputs it recives\n",
    "    \n",
    "<img src=\"res/Binary_Node.png\" width = 70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the simplest neural network Structures\n",
    "- Requires a slightly different artificial neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear Threashold Unit (LTU) \n",
    "<img src=\"res/LTU_node.png\" width = 30% align= \"center\">\n",
    "\n",
    "- inputs and output: numbers (rather than binary)\n",
    "\n",
    "- each input has an associated weight\n",
    "\n",
    "- an activation function is used to determine the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###### How an LTU computes its output\n",
    "- \n",
    "    1. First the node calculates the input (z) by adding each input value multiplied by its weight \n",
    "     \\begin{equation*}\n",
    "    Z = \\sum_{k=1}^n \\left(Input_k * Weight_k  \\right)\n",
    "    \\end{equation*}\n",
    "    2. Next, the node passes z into its activation function. In this case, we are using the step function.  \n",
    "    <img src=\"res/step_func.png\" width = 30%> \n",
    "    \\begin{equation*}\n",
    "    step(x) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        0 & \\quad x < 0 \\\\\n",
    "        1 & \\quad x > 0\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "    \\end{equation*}\n",
    "\n",
    "    3. Finally, the node passes step(z) as its output \n",
    "    ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Perceptron Architecture \n",
    "- One of the simplest Artificial Neural Network Architectures\n",
    "- A neural network made of a single layer of LTU's and an extra bias node\n",
    "<img src=\"res/Perseptron.png\" width = 50%> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training a Perceptron\n",
    "- \"Cells that fire together, wire together\" - *Sigrid Löwel*\n",
    "- Perceptrons are trained using a variant of this rule which account for the errors made by the network\n",
    "    - does not reinforce connections that lead to a wrong output\n",
    "    - one training instance is feed into the model at a time, for each instance it makes a prediction\n",
    "    - For every output neuron that produced the wrong answer, it reinforces the connection weights from the inputs that would have contributed to the proper result\n",
    "    \n",
    "   \\begin{equation*}\n",
    "    w_{i,j}^\\left(post\\right) = w_{i,j}^\\left(pre\\right) + η \\left( \\hat y_j - y_j \\right)x_i\n",
    "    \\end{equation*}\n",
    "    \n",
    "        - w<sub>i, j</sub><sup>(pre)</sup> is the connection weight between the ith input neuron and the jth output neuron before training instance.\n",
    "        - x<sub>i</sub> is the ith input value of the current training instance.\n",
    "        - ŷ<sub>j</sub> is the output of the jth output neuron for the current training instance.\n",
    "        - y<sub>j</sub> is the target output of the jth output neuron for the current training instance. • η is the learning rate. \n",
    "        - η is the learning rate constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In practice, it became apparent that a single layer perceptron was incapable of trivial problems (such as the XOR shown in figure 1.2)\n",
    "- As it turns out, this limitation can be addressed by making the perceptron a bit more complicated\n",
    "\n",
    "###### Mult-layer Perceptron\n",
    "<img src=\"res/Multi-Layer_Perseptron.png\" width = 50% align= \"right\">\n",
    "Unlike the original perceptron, the MLP is comprized of 3 layers:\n",
    "   \n",
    "   1. **The input Layer**\n",
    "       - containing all the inputs that are coming into the model \n",
    "       ---\n",
    "   2. **The Hidden Layer**\n",
    "       - a layer of LTU's that take in the input layer\n",
    "       - each node passes a result up to the next layer\n",
    "       - this layer can be made up of many different layers of LTU's\n",
    "           - if a model has ≥ 2 hidden layers, it is called a *Deep Neural Network* (DNN)\n",
    "       ---\n",
    "   3. **The Output Layer**\n",
    "       - a final layer of LTU's that is densely connected to the hidden layer bellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Backpropagation\n",
    "- For each training instance, the algorithm feeds it into the network \n",
    "    - computes the output for each consecutive layer\n",
    "- The final predicted value is compared to the previous version expected. The error is computed\n",
    "- The network is then traversed backwards\n",
    "    - for each node in layer i, it looks at how each node in layer i-1 contributed to its error (Gradient Descent)\n",
    "    - then it adjusts the weights to compensate for this error\n",
    "    - This repeats until it reaches the input layer\n",
    "\n",
    "- In order to calculate the error of each node, we have to use a continuous activation function\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MLP For Classification Problems\n",
    "<img src=\"res/classification.gif\" width = 50%> \n",
    "\n",
    "\n",
    "- One common use for multi-layered perceptrons is classification\n",
    "    - requires cases to be exclusive (limited set of possible outputs) \n",
    "- The output layer is often modified by replacing individual activation functions with one shared softmax function\n",
    "    - The output of each neuron is the estimated probability of the corresponding class\n",
    "\n",
    "<img src=\"res/classification_MLP.png\" width = 50%> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When training a Neural Network, there a number of Hyperparameters that can be tweaked to improve performance and speed\n",
    "- There exist algorithms (for example gridsearch) to fine tune these parameters but this is often slow\n",
    "    - a randomized search (or tools such as Oscar) are able to find a good set of parameters relatively quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For many problems, a single hidden layer is enough to get reasonable results\n",
    "    - an MLP with just one hidden layer can model even the most complex functions, given that it has enough nodes per layer\n",
    "    \n",
    "    \n",
    "- Deep networks have a much higher parameter efficiency than shallower ones\n",
    "    - they can model complex functions with using exponentially less nodes, making them faster to train\n",
    "    - DNNs take advantage of datasets with a hierarchical structure\n",
    "        - **lower layers** model the low-level structure of the data (e.g line segments in an image)\n",
    "        - **middle layers** combine the low_level structures (e.g. grouping line segments into shapes)\n",
    "        - **higher layers** combine these intermediet structures into higher-level ones (e.g. faces, object, logos, ect...)\n",
    "        \n",
    "        <img src=\"res/dnn_facial.png\" width = 80% align=left>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For more complicated problems, a good idea is to test a smaller number of layers and start adding more until you start overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Neurons per Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A common practice used to be to create a funnel like structure\n",
    "    - more nodes lower layers that taper off into less nodes higher layers\n",
    "    - rational being that many lower level features would join into fewer higher level features\n",
    "- This practice isn't as common now\n",
    "    - many models just use the same number of nodes per layer\n",
    "- One way of finding out how many nodes you need per layer is to start with a smaller count and build up your model until the network starts overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In many cases ReLU is used in hidden layers since it is faster to compute and gradient decent is less likely to get stuck in a plateau\n",
    "- For outer layer\n",
    "    - softmax is a good choice for classification problems\n",
    "    - for regression tasks, its better to have no activation function at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Common examples:\n",
    " <img src=\"res/Activation Functions.png\" width = 50% align=right> \n",
    "*Logistic Function*\n",
    "\\begin{equation*} \n",
    "    \\sigma\\left(z\\right) = \\dfrac{1}{1+e^{-z}}\n",
    " \\end{equation*}\n",
    " \n",
    "*Hyperbolic Tangent Function* \n",
    "\\begin{equation*} \n",
    "    tanh\\left(z\\right) = 2\\sigma\\left( 2z \\right)-1\n",
    " \\end{equation*}\n",
    " \n",
    "*ReLU Function* \n",
    "\\begin{equation*} \n",
    "    ReLU\\left(z\\right) = max\\left(0, z\\right)\n",
    " \\end{equation*}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
